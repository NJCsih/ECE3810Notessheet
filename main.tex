%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Nelson Crane
% April 30, 2024
% Notes Sheet for ECE 3810 Final
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Lachaise Assignment
% LaTeX Template
% Version 1.0 (26/6/2018)
%
% This template originates from:
% http://www.LaTeXTemplates.com
%
% Authors:
% Marion Lachaise & François Févotte
% Vel (vel@LaTeXTemplates.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

%\documentclass{article}
\documentclass[letterpaper,landscape]{article}

\input{structure.tex} % Include the file specifying the document structure and custom commands


\begin{document}
\begin{multicols}{2}

%----------------------------------------------------------------------------------------

\section{Ch 1 stuff}
\begin{itemize}
	\item $\text{[Speedup]} = \frac{\text{[Latency 1]}}{\text{[Latency 2]}} = \frac{\text{[Throughput 1]}}{\text{[Throughput 2]}}$
	\item $\text{[Performance Improvement]} = \text{[Speedup]} - 1$
	\item $\text{[Exec. Time]} = \text{[Instr. Count]} \times \text{[CPI]} \times \text{[Clock Speed]}$
	\item $\text{[Performance]} = \text{[Execution Time]}^{-1}$
	\item $\text{[Dynamic Power]} \propto \text{[Activity]} \times \text{[Capacitance]} \times \text{[Voltage]}^2 \times \text{[Frequency]}$
\end{itemize}


\section{ASM}
\begin{itemize}
	\item Calle saves \$s\# registers, caller saves everything else
	\item Stack grows down
\end{itemize}
\begin{verbatim}
.data
prompt:          .asciiz "Ente...rs: \n"
input:           .space 81
inputSize:       .word 80
exitMessage_p1:  .asciiz "Word count: "
.text
\end{verbatim}


\section{Binary Stuff}
\subsection*{Float}

"But we're not dealing with real numbers, this is floating point baby!"
\begin{itemize}
	\item Dec from Float: $(-1)^{\text{[Sign bit]}} \times (1 + \text{[Mantissa]}) \times 2^{(\text{[Exponent Bits] - \text{[Bias]}})}$
	\item Float from Dec: Convert to bin sci notation, 'sub \[the bias\] \[to get\] in' to floating exponent land
	\item Dec to F32: 
	\item Zero:     "00000000" exponent, all zero mantissa, sign as usual
	\item Inf:      "11111111" exponent, all zero mantissa, sign as usual
	\item NANs:     "11111111" exponent, sign and mantissa "left to implementer's discretion"
	\item Subnorms: "00000000" exponent, then replace the leading 1 with a zero and continue as usual. Getting increasingly smaller but less precise
\end{itemize}

Exponent chart from jan masali:

\begin{Figure}
	\centering
	\includegraphics[width=\linewidth]{Figures/FloatExponentsChart.png}
\end{Figure}

\subsection*{Demorgans and Boolean algebra}
\begin{itemize}
	\item Xors are 1 if there's an even number of 1s, 0 if an even number. -- Thus 'Parity'
	\item $\lnot( A \lor B ) = \lnot( A ) \land \lnot( B )$
	\item $\lnot( A \land B ) = \lnot( A ) \lor \lnot( B )$
	\item You can flip the operation and flip weather they're internally or externally negated.
\end{itemize}


\section{FSM}
Not that hard, just like, make sure all states and lines can be justified and are labeled.


\section{Pipelining (oh god)}
\subsection*{Usual steps of a 5 stage pipeline}
\begin{itemize}
	\item IF Instruction fetch \-\- 1 cycle, get next instruction from InstrMem or cache
	\item DR Data read from register
	\item AL ALU, does the computation
	\item DM Data memory: 
	\item RW Read Write
\end{itemize}
\subsection*{Bypassing}
\begin{itemize}
	\item Point of production:
	\subitem add, sub, etc: end of ALU
	\subitem lw: end of DM -- the mem it's reading into register
	\item Point of consumption:
	\subitem add, sub, lw: start of ALU
	\subitem sw/lw \$1, 8(\$2):
	\subsubitem start of ALU for \$2
	\subsubitem start of DM for \$1
\end{itemize}


\section{Cache}
\begin{Figure}
	\centering
	\includegraphics[width=\linewidth]{Figures/CacheAddressDiagram.png}
\end{Figure}

\begin{itemize}
	\item Offset and Index need the bits they need, tag gets the rest
	\subitem Offset is for within the block
	\subitem Index: which set we're referencing
	\item Tag array size = sets * ways * tagSize
	\item $\text{[Offset]} = \text{address} \% \text{[block size in bytes]}$
	\item $\text{[Index]} = (\text{address} / \text{[block size in bytes]}) \% \log_2(\text{Sets Count})$
	\item $\text{[Tag]} = \text{address} / (\text{[block size in bytes]} * \log_2(\text{[Sets Count]}))$
	\item Tag bits: remaining bits
	\item Offset bits: depending on blocksize, determines what byte within the block is being referenced log_2(blocksize (B))
	\item Index bits: determined by amount of sets log_2(set count)
	\item Usually we write-allocate, briging a miss into cache. Read misses always bring block into cache
	\subitem Usually we evict the least-recently used block
	\item Bit string: tag index offset
\end{itemize}


\section{Spectre/Meltdown}
\begin{itemize}
	\item "Spectre refers to a whole family of potential weaknesses of which meltdown is just one" - Computerphile video description
	\item These are the result of a combination of speculative execution and out of order execution
	\item Meltdown is a specific kernel memory exploit
	\item Need a: "lw \$t1, 0(secret); lw \$t2, \$t1" series of two instructions to leave footprints in cache
\end{itemize}


\section{Virtual Memory}
\begin{itemize}
	\item Memory virtualised by the kernel, the program thinks it has a single uninterrupted area to work with
	\item Programs have a pagetable, which is a list of how virtual pages are mapped to physical pages in memory
	\item Page table translations are mostly done via the Translation Lookaside Buffer, which falls back to the pagetable
\end{itemize}

\begin{itemize}
\end{itemize}


\section{Multiprocessor Design}
\begin{itemize}
	\item Each core has a cache, the LLC is shared
	\item TODO: Protocol descripions and examples of that chart
\end{itemize}
\subsection*{Cache Coherence Protocls}
\begin{itemize}
	\item Directory-based: A single location (directory) keeps track of the sharing status of a block of memory
	\item Snooping: Every cache block is accompanied by the sharing status of that block - all cache controllers monitor the shared bus so they can update the sharing status of the block, if necessary
	\item Write invalidate: a processor gains exclusive access of a block before writing by invalidating all other copies
	\item Write-update: when a process or writes, it updates other shared copies of that block~
\end{itemize}
\subsection*{Locks}
\begin{itemize}
	\item Atomic exchange: simulatneous load and store operation, locks memory at the same moment it reads. -- locks it while it transfers into register
\end{itemize}
\subsection*{MP exmaple}
\begin{Figure}
	\centering
	\includegraphics[width=\linewidth]{Figures/MultiprocessorMemoryExample.png}
\end{Figure}


\section{Basic structure of a GPU:}
\begin{itemize}
	\item many Single Instruction Multiple Thread cores, each with several warps and a warp scheduler. -- large cache
	\item Very quick to abandon the current project if it stalls and start work on the next -- minimal downtime, easy to context switch
	\item Each SIMT core has priavate L1 cache, large L2 shared by all cores. Each L2 bank services a subset of addresses
	\item The 
\end{itemize}


\section{Disk stuff:}
\begin{itemize}
	\item 1-12 platters (glass disks), 5-30k tracks (rings), 100-500 sectors, ~512B/sector (circle around a track)
	\item Arm moving to correct track $\Rightarrow$ seek time, ~5-12ms, maybe less
	\item Rotational latency: time to rotate sector under head, usually ~2ms
	\item Transfer time: time taken to transfer a block of bits out of disk, usually 3-65 MB/s
	\item Disk controller: maintains disk cache, sets up transfers.
	\item Mean time to Failure/Restore
	\subitem Reliability $\rightwhitearrow$ MTTF
	\subitem Availability: fraction of time service matches specs, MTTF / (MTTF + MTTR)
\end{itemize}


\section{Raid types overview:}
\begin{itemize}
	\item Raid 0: No redundancy, stripes disks across drives to improve parallelism. 
	\item Raid 1: Mirrors all disks, can sometimes increase read speed, highly redundant
	\item Raid 2: bit level striping, requires lockstep drives. Lots of parity drives
	\item Raid 3: byte level striping with dedicated parity disk. Highest sequential read speeds. Usually requires lockstep drives.
	\item Raid 4 and 5: block level striping, 4 has a dedicated parity disk, 5 distributes parity sections among drives.
	\item Raid 6: Same as raid 5, but has two parity blocks per stripe, again distributed among drives. Can work even after two drive failures. Technically raid 6 can have an arbitrary number of parity blocks added for increased redundancy.
\end{itemize}





\end{multicols}
\end{document}
